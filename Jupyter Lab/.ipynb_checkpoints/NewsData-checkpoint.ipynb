{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tejaswini Reddy Boda 5024 3913\n",
    "    TEAM MATE: Pretty Mary Philip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_articles(articles):\n",
    "    articalArray = []\n",
    "    if 'response' in articles:\n",
    "        for i in articles['response']['docs']:\n",
    "            article = {}\n",
    "            article['id'] = i['_id']\n",
    "            article['headline'] = i['headline']['main'].encode(\"utf8\")\n",
    "            article['date'] = i['pub_date'][0:10] # cutting time of day.\n",
    "            if i['snippet'] is not None:\n",
    "                article['snippet'] = i['snippet'].encode(\"utf8\")\n",
    "            article['url'] = i['web_url']\n",
    "            article['word_count'] = i['word_count']\n",
    "            articalArray.append(article)\n",
    "    return(articalArray) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles(date,query):\n",
    "    all_articles = []\n",
    "    for i in range(0,100): #NYT limits pager to first 100 pages. But rarely will you find over 100 pages of results anyway.\n",
    "        articles = api.search(q = query,\n",
    "               begin_date = date,\n",
    "               page = str(i))\n",
    "        #articlesP = parse_articles(articles)\n",
    "        all_articles = all_articles + articles\n",
    "    return(all_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nytimesarticle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c08ec80ecf0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnytimesarticle\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marticleAPI\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0murllib2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticleAPI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'46f13d18c782467da9700dd2b660183d'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nytimesarticle'"
     ]
    }
   ],
   "source": [
    "#NYTimes API\n",
    "import csv\n",
    "import json\n",
    "from nytimesarticle import articleAPI\n",
    "import urllib.request as urllib2\n",
    "api = articleAPI('46f13d18c782467da9700dd2b660183d')\n",
    "#articles = api.search( q = 'gun control', begin_date = 20180330 )\n",
    "#print (articles)\n",
    "#articles = get_articles('gun control', 20180330)\n",
    "all_articles = []\n",
    "for i in range(0,100): #NYT limits pager to first 100 pages. But rarely will you find over 100 pages of results anyway.\n",
    "    articles = api.search(q = 'north korea',\n",
    "           begin_date = 20180330,\n",
    "           page = i)\n",
    "    print(articles)\n",
    "    articlesP = parse_articles(articles)\n",
    "    print(articlesP)\n",
    "    all_articles = all_articles + articlesP\n",
    "#print (articles)\n",
    "\n",
    "#f=csv.writer(open('nyTest.csv','wb+'))\n",
    "\n",
    "#for item in all_articles:\n",
    "#  f.writerow([item['id'], item['date'], item['url']])\n",
    "\n",
    "#download_dir = \"nyTest.csv\" #where you want the file to be downloaded to \n",
    "#csv = open(download_dir, \"w\") \n",
    "#\"w\" indicates that you're writing strings to the file\n",
    "#csv.write(articles)\n",
    "#print(all_articles)\n",
    "#keys = all_articles[0].keys()\n",
    "#print(keys)\n",
    "# with open('nyTest.csv', 'wb') as output_file:\n",
    "#     dict_writer = csv.DictWriter(output_file, keys)\n",
    "#     #dict_writer.writeheader()\n",
    "#     csvwriter.writerow(keys)\n",
    "#     dict_writer.writerows(all_articles)\n",
    "artCSV = open('nytArticles-03-30.csv', 'w')\n",
    "csvwriter = csv.writer(artCSV)\n",
    "count = 0\n",
    "for art in all_articles:\n",
    "    if count == 0:\n",
    "         header = art.keys()\n",
    "         csvwriter.writerow(header)\n",
    "         count += 1\n",
    "    #print(art.values())\n",
    "    csvwriter.writerow(art.values())\n",
    "artCSV.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following command must be run outside of the IPython shell:\n",
      "\n",
      "    $ pip install nytimesarticle\n",
      "\n",
      "The Python package manager (pip) can only be used from outside of IPython.\n",
      "Please reissue the `pip` command in a separate terminal or command prompt.\n",
      "\n",
      "See the Python documentation for more informations on how to install packages:\n",
      "\n",
      "    https://docs.python.org/3/installing/\n"
     ]
    }
   ],
   "source": [
    "pip install nytimesarticle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import csv\n",
    "#ny_article=\"https://www.nytimes.com/2018/03/30/us/vermont-gun-law.html\"\n",
    "#page = urllib2.urlopen(ny_article)\n",
    "#soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    #story = soup.findAll(text=True)\n",
    "    story = soup.findAll('p')\n",
    "    #visible_texts = filter(tag_visible, story)  \n",
    "    #return u\" \".join(t.strip() for t in visible_texts)\n",
    "    u = \"\"\n",
    "    for t in story:\n",
    "        u = u + t.getText()\n",
    "    return u\n",
    "\n",
    "write_file = open('nytArticles-30.txt', 'w')\n",
    "\n",
    "with open('nytArticles-03-30.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        ht = urllib2.urlopen(row['url']).read()\n",
    "        #print(text_from_html(ht))\n",
    "        write_file.write(text_from_html(ht))\n",
    "        write_file.write('\\n\\n')\n",
    "write_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
